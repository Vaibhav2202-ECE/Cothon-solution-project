{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T13:05:32.821767Z",
     "iopub.status.busy": "2024-08-27T13:05:32.821022Z",
     "iopub.status.idle": "2024-08-27T13:05:45.695111Z",
     "shell.execute_reply": "2024-08-27T13:05:45.693574Z",
     "shell.execute_reply.started": "2024-08-27T13:05:32.82169Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%pip install openai chromadb==0.3.29 langchain tiktoken\n",
    "%pip install -U langchain-community\n",
    "%pip install faiss-cpu\n",
    "%pip install sentence-transformers\n",
    "\n",
    "# Import necessary libraries after resolving conflicts and installing dependencies\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from transformers import pipeline\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "# Import necessary libraries\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "# List input data files under the input directory\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple chatbot using Google model from Huggingface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T13:05:45.697853Z",
     "iopub.status.busy": "2024-08-27T13:05:45.696843Z",
     "iopub.status.idle": "2024-08-27T13:07:11.612243Z",
     "shell.execute_reply": "2024-08-27T13:07:11.61084Z",
     "shell.execute_reply.started": "2024-08-27T13:05:45.697793Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "# Choose and load a Hugging Face model for text generation\n",
    "model_name = \"google/flan-t5-large\"  # Model suitable for text-to-text generation tasks\n",
    "hf_pipeline = pipeline(\"text2text-generation\", model=model_name, clean_up_tokenization_spaces=True)\n",
    "\n",
    "# Wrap the Hugging Face pipeline using LangChain's HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "# Create a PromptTemplate\n",
    "prompt_template = PromptTemplate(input_variables=[\"question\"], template=\"{question}\")\n",
    "\n",
    "# Create a LangChain LLMChain with the prompt\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# Run the chain with an input\n",
    "response = chain.run({\"question\": \"What is the capital of the USA?\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T13:07:11.614319Z",
     "iopub.status.busy": "2024-08-27T13:07:11.613945Z",
     "iopub.status.idle": "2024-08-27T13:07:19.460273Z",
     "shell.execute_reply": "2024-08-27T13:07:19.458273Z",
     "shell.execute_reply.started": "2024-08-27T13:07:11.614282Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create Sample Text Documents as LangChain Documents\n",
    "sample_texts = [\n",
    "    \"Washington, D.C., formally the District of Columbia, is the capital city of the United States. It is located on the east bank of the Potomac River and was founded after the American Revolution to serve as the nation's capital.\",\n",
    "    \"The United States of America (USA) is a country primarily located in North America. It consists of 50 states, a federal district, and several territories.\",\n",
    "    \"San Francisco is a major city in California, known for its Golden Gate Bridge, cable cars, and colorful Victorian houses.\"\n",
    "]\n",
    "\n",
    "# Convert sample texts into LangChain Document objects\n",
    "documents = [Document(page_content=text) for text in sample_texts]\n",
    "\n",
    "# Chunk Documents\n",
    "# Split documents into chunks for better retrieval performance\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "chunked_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "# Index the Documents\n",
    "# Use Hugging Face embeddings for indexing\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create an index using FAISS for efficient similarity search\n",
    "index = FAISS.from_documents(chunked_documents, embedding_model)\n",
    "\n",
    "# Set Up the RAG Pipeline\n",
    "# Choose and load a Hugging Face model for text generation\n",
    "model_name = \"google/flan-t5-large\"  # Model suitable for text-to-text generation tasks\n",
    "hf_pipeline = pipeline(\"text2text-generation\", model=model_name)\n",
    "\n",
    "# Wrap the Hugging Face pipeline using LangChain's HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "# Create a LangChain RetrievalQA chain that combines document retrieval with text generation\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # 'stuff' or 'map_reduce' \n",
    "    retriever=index.as_retriever()\n",
    ")\n",
    "\n",
    "#Run the RAG Chain with an Input Question\n",
    "response = rag_chain.run({\"query\": \"What is the capital of the USA?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next we will be using Chroma DB to store vectorised documents and also we will use SimpleSequentialChain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T13:12:53.014847Z",
     "iopub.status.busy": "2024-08-27T13:12:53.013614Z",
     "iopub.status.idle": "2024-08-27T13:13:00.732429Z",
     "shell.execute_reply": "2024-08-27T13:13:00.731073Z",
     "shell.execute_reply.started": "2024-08-27T13:12:53.014794Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Create Sample Text Documents as LangChain Documents\n",
    "sample_texts = [\n",
    "    \"Washington, D.C., formally the District of Columbia, is the capital city of the United States. It is located on the east bank of the Potomac River and was founded after the American Revolution to serve as the nation's capital.\",\n",
    "    \"The United States of America (USA) is a country primarily located in North America. It consists of 50 states, a federal district, and several territories.\",\n",
    "    \"San Francisco is a major city in California, known for its Golden Gate Bridge, cable cars, and colorful Victorian houses.\",\n",
    "    \"Harry Potter is a series of seven fantasy novels written by British author J.K. Rowling. The series chronicles the lives of a young wizard, Harry Potter, and his friends as they attend Hogwarts School of Witchcraft and Wizardry.\"\n",
    "]\n",
    "\n",
    "# Convert sample texts into LangChain Document objects\n",
    "documents = [Document(page_content=text) for text in sample_texts]\n",
    "\n",
    "# Chunk Documents\n",
    "# Split documents into chunks for better retrieval performance\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "chunked_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "# Index the Documents using ChromaDB\n",
    "# Use Hugging Face embeddings for indexing\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize ChromaDB vector store\n",
    "chroma_db = Chroma.from_documents(documents=chunked_documents, embedding=embedding_model)\n",
    "\n",
    "# Create a retriever from ChromaDB\n",
    "retriever = chroma_db.as_retriever()\n",
    "\n",
    "#  Set Up the RAG Pipeline\n",
    "# Choose and load a Hugging Face model for text generation\n",
    "model_name = \"google/flan-t5-large\"  # Model suitable for text-to-text generation tasks\n",
    "hf_pipeline = pipeline(\"text2text-generation\", model=model_name, clean_up_tokenization_spaces=True)  # Set to avoid FutureWarning\n",
    "\n",
    "# Wrap the Hugging Face pipeline using LangChain's HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "# Create a PromptTemplate for generating responses\n",
    "prompt_template = PromptTemplate(input_variables=[\"context\", \"question\"], template=\"{context}\\n\\n{question}\")\n",
    "\n",
    "# Chain Setup: Chain should properly retrieve and pass context\n",
    "def retrieve_and_format(question):\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "    context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "    return {\"context\": context, \"question\": question}\n",
    "\n",
    "# Setup the pipeline correctly using components\n",
    "chain = (\n",
    "    RunnablePassthrough()  # This will pass the input question to the next step\n",
    "    | retrieve_and_format  # Retrieve and format documents to create context\n",
    "    | prompt_template  # Use the prompt template to structure the input\n",
    "    | llm  # Generate a response using the language model\n",
    "    | StrOutputParser() # Parse the output as a string\n",
    ")\n",
    "\n",
    "# Run the Chain Pipeline with the New Question\n",
    "question = \"Who is the author of Harry Potter?\"\n",
    "response = chain.invoke(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concept of Memory** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T13:13:11.818323Z",
     "iopub.status.busy": "2024-08-27T13:13:11.817851Z",
     "iopub.status.idle": "2024-08-27T13:13:11.824472Z",
     "shell.execute_reply": "2024-08-27T13:13:11.822951Z",
     "shell.execute_reply.started": "2024-08-27T13:13:11.818281Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T13:13:12.214685Z",
     "iopub.status.busy": "2024-08-27T13:13:12.21365Z",
     "iopub.status.idle": "2024-08-27T13:13:12.222567Z",
     "shell.execute_reply": "2024-08-27T13:13:12.221107Z",
     "shell.execute_reply.started": "2024-08-27T13:13:12.214631Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "memory1 = ConversationBufferMemory( memory_key=\"history\") \n",
    "\n",
    "memory1.save_context({\"input\": \"hey\"}, {\"output\": \"wassup\"}) \n",
    "\n",
    "print(f\"memory1 ==> {memory1.load_memory_variables({})}\")\n",
    "\n",
    "memory2 = ConversationBufferMemory( memory_key=\"chat_history\") \n",
    "\n",
    "memory2.save_context({\"input\": \"hi\"}, {\"output\": \"whats are you doing?\"}) \n",
    "\n",
    "print(f\"memory2 ==> {memory2.load_memory_variables({})}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T13:13:12.995679Z",
     "iopub.status.busy": "2024-08-27T13:13:12.994581Z",
     "iopub.status.idle": "2024-08-27T13:13:16.407922Z",
     "shell.execute_reply": "2024-08-27T13:13:16.406817Z",
     "shell.execute_reply.started": "2024-08-27T13:13:12.995615Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Load the Hugging Face LLM model\n",
    "hf_pipeline = pipeline('text-generation', model='gpt2', max_length=256, max_new_tokens=50)\n",
    "\n",
    "# Wrap the pipeline in a HuggingFacePipeline for compatibility with LangChain\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "# Set up the embedding model for vector storage\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Sample documents for testing\n",
    "documents = [\n",
    "    Document(page_content=\"Paris is the capital of France.\"),\n",
    "    Document(page_content=\"France is a country in Europe.\"),\n",
    "    Document(page_content=\"The French Revolution was a period of social and political upheaval in France.\")\n",
    "]\n",
    "\n",
    "#  Embed the documents\n",
    "embedded_docs = [(doc, embedding_model.embed_query(doc.page_content)) for doc in documents]\n",
    "doc_texts = [doc.page_content for doc in documents]\n",
    "embeddings = [embedding_model.embed_query(text) for text in doc_texts]\n",
    "\n",
    "#  Initialize the FAISS index\n",
    "dimension = len(embeddings[0])  # Assuming all embeddings have the same length\n",
    "index = faiss.IndexFlatL2(dimension)  # Using L2 (Euclidean) distance\n",
    "index.add(np.array(embeddings))\n",
    "\n",
    "# Initialize the FAISS vector store with InMemoryDocstore\n",
    "docstore = InMemoryDocstore(dict(enumerate(documents)))\n",
    "vector_store = FAISS(embedding_function=embedding_model, index=index, docstore=docstore, index_to_docstore_id={i: i for i in range(len(documents))})\n",
    "\n",
    "# Initialize memory buffer with explicit output key\n",
    "memory = ConversationBufferMemory(output_key='result')  # Set output_key to 'result'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T13:13:16.410751Z",
     "iopub.status.busy": "2024-08-27T13:13:16.410314Z",
     "iopub.status.idle": "2024-08-27T13:13:16.420463Z",
     "shell.execute_reply": "2024-08-27T13:13:16.419144Z",
     "shell.execute_reply.started": "2024-08-27T13:13:16.410707Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#  Create a retrieval-based QA chain\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Attach memory buffer to the chain\n",
    "qa_chain.memory = memory\n",
    "\n",
    "# Define a function to use the QA model with memory buffer\n",
    "def query_with_memory(query: str):\n",
    "    # Run the retrieval-based QA chain and get the results\n",
    "    result = qa_chain({\"query\": query})\n",
    "    \n",
    "    # Extract the desired output ('result') to save in memory\n",
    "    output_for_memory = {\"result\": result['result']}\n",
    "    \n",
    "    # Save only the relevant output key to memory\n",
    "    memory.save_context({\"query\": query}, output_for_memory)\n",
    "    \n",
    "    # Retrieve memory for debugging or review\n",
    "    memory_content = memory.load_memory_variables({})\n",
    "    \n",
    "    print(\"Memory Buffer Content:\", memory_content)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-27T13:13:16.423131Z",
     "iopub.status.busy": "2024-08-27T13:13:16.422551Z",
     "iopub.status.idle": "2024-08-27T13:13:20.641623Z",
     "shell.execute_reply": "2024-08-27T13:13:20.64029Z",
     "shell.execute_reply.started": "2024-08-27T13:13:16.423074Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Test the implementation\n",
    "query = \"What is the capital of France?\"\n",
    "response = query_with_memory(query)\n",
    "print(\"Response:\", response['result'])  # Use 'result' to match the correct key\n",
    "\n",
    "# Add more queries to see memory buffer in action\n",
    "query = \"Tell me more about its history.\"\n",
    "response = query_with_memory(query)\n",
    "print(\"Response:\", response['result'])  # Use 'result' to match the correct key"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30761,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
